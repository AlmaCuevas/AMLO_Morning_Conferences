{
 "cells": [
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "markdown",
   "source": [
    "# Analysing text similarity using spaCy, networkX \n",
    "\n",
    "This notebook demonstrates one way of using spaCy to conduct a rapid thematic analysis of a small corpus of comments, and introduces some unusual network visualisations.\n",
    "Topics include: \n",
    "* [spaCy](https://spacy.io/) - an open source NLP library, \n",
    "* word vectors, and\n",
    "* networkX - an open source network (graph) analysis and visualisation library. \n",
    "\n",
    "The notebook is partly a reminder for myself on just how (well) these techniques work, but I hope that others find it useful. I'll continue to update it with more techniques over the coming weeks.\n",
    "If you have any suggestions, feel free to make them in the comments, fork the notebook etc. I'm keen to exchange tips and tricks. \n"
   ]
  },
  {
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "collapsed": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": false
   },
   "cell_type": "markdown",
   "source": [
    "# Plan"
   ]
  },
  {
   "metadata": {
    "_uuid": "92c78bfb0d1c6b329350126b7d02c435c7811a2a"
   },
   "cell_type": "markdown",
   "source": [
    "* load a representative set of tweets\n",
    "* demonstrate some basic spaCy features\n",
    "* test its similarity metrics\n",
    "* build a graph data structure for storing (n * n-1) / 2 similarity results\n",
    "* visualise the clusters of most-similar items in the data\n",
    "* plan the next steps"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "1baba93a71fccd11a08c3f154a142acc6cf55827"
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx                        # a really useful network analysis library\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "# from networkx.algorithms import community   # not used, yet... \n",
    "import datetime                              # access to %%time, for timing individual notebook cells\n",
    "import os"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "1d5064ef9e517d4a06ff19a9fab51612a5d91c04"
   },
   "cell_type": "markdown",
   "source": [
    "This next step load the spaCy language model. It generally takes about 13s to load this 'large' model."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "5f3a53974d0953764137b78fa7f52f90e4139b90"
   },
   "cell_type": "code",
   "source": [
    "#nlp = spacy.load('es_core_news_lg')           # A more detailed model (with higher-dimension word vectors) - 13s to load, normally\n",
    "nlp = spacy.load('es_core_news_md')           # a smaller model, e.g. for testing"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "d6bdb4a056364bc4fbbaf90e7fbbf5e7eee39f14"
   },
   "cell_type": "code",
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]  # makes the output plots large enough to be useful"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "01dbb00da7ac0311a5f72e2c55150b18589f8753"
   },
   "cell_type": "markdown",
   "source": [
    "## Data"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "1dd159bf91fb20efecba5995c86e7030aa503d35"
   },
   "cell_type": "code",
   "source": [
    "rowlimit = 500              # this limits the tweets to a manageable number\n",
    "data = pd.read_csv('all_df.csv', nrows = rowlimit)\n",
    "data.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "a69dcaf3512a6982d8836534fb2054958da58e3c"
   },
   "cell_type": "code",
   "source": [
    "data.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "f84dd62f73e6de89011bf335a0ef04d668edde36"
   },
   "cell_type": "markdown",
   "source": [
    "## Using spaCy to parse the tweets."
   ]
  },
  {
   "metadata": {
    "_uuid": "7fac66540684141b588d9db832e543b22c9f7967"
   },
   "cell_type": "markdown",
   "source": [
    "N.B. this next step can take a while - e.g. 14 mins, for the full set - but only 5s for 500 rows.\n",
    "\n",
    "(based on https://stackoverflow.com/questions/44395656/applying-spacy-parser-to-pandas-dataframe-w-multiprocessing)..."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "18b58394dbd6f69fa973b4facf6fa86f50912612"
   },
   "cell_type": "code",
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "parsed_doc = [] \n",
    "col_to_parse = 'Texto'\n",
    "\n",
    "for doc in nlp.pipe(data[col_to_parse].astype('unicode').values, batch_size=50,\n",
    "                        n_threads=3):\n",
    "    if doc.is_parsed:\n",
    "        parsed_doc.append(doc)\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "\n",
    "\n",
    "data['parsed_doc'] = parsed_doc\n",
    "data['comment_tokens'] = tokens\n",
    "data['comment_lemma'] = lemma\n",
    "data['pos_pos'] = pos"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "2430fb7633d522fcbc89497b4ec625f0a382d4d0"
   },
   "cell_type": "markdown",
   "source": [
    "## Basic checks of the parsed data"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "869cd785398dbd6f54a5c5b7ddb3d00e410fadf2"
   },
   "cell_type": "code",
   "source": [
    "data.head(8)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "8fa025df4678e97ecf53af85c938fd9eb6350984"
   },
   "cell_type": "markdown",
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "metadata": {
    "_uuid": "5b31729ff59a6db3e9797640829966fdd34cb747"
   },
   "cell_type": "markdown",
   "source": [
    "We could reduce increase the signal:noise ratio in these texts by removing some of the more common words (or *stopwords*). By removing these from the tweets, we would prevent them from influencing the analysis of whether two tweets are similar. I'm not addressing this is the notebook yet, but I will come back to it later. For now, let's just look at what words are included in spaCy's stopword list."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "13fd50fec79c853699083e3cdff8f347bcd57317"
   },
   "cell_type": "code",
   "source": [
    "stop_words = set(stopwords.words('spanish'))\n",
    "print('Number of stopwords: %d' % len(stop_words))\n",
    "print(list(stop_words))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "05870fb02c805e892c30864a9ec78d579a85e86a"
   },
   "cell_type": "markdown",
   "source": [
    "## Testing spaCy's similarity function"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "0fbeb19147fc155f2061d55143f51700e9caa522"
   },
   "cell_type": "code",
   "source": [
    "print(data['parsed_doc'][0].similarity(data['parsed_doc'][1]))\n",
    "print(data['parsed_doc'][0].similarity(data['parsed_doc'][10]))\n",
    "print(data['parsed_doc'][1].similarity(data['parsed_doc'][10]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "6a589e6d14de2e10713511bda9822bdd693e4bf8"
   },
   "cell_type": "markdown",
   "source": [
    "If you've limited the rows imported, then you may only have Democrat tweets (which occur first in the list)."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "ab92e949e07c3b9975d73bcceaf907d083a597ee"
   },
   "cell_type": "code",
   "source": [
    "data.Party.unique()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "bbbb900e81f0bf33d1e76879089ad571f04604d4"
   },
   "cell_type": "code",
   "source": [
    "world_data = data\n",
    "#world_data = data[data.Party == 'Democrat']      # or use either of these, if you want to see tweets from only one party\n",
    "#world_data = data[data.Party == 'Republican']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "92ff84edbbe3f898291e5b205802cba403352d4d"
   },
   "cell_type": "code",
   "source": [
    "# takes 1s for 500 nodes - but of course this won't scale linearly!                              \n",
    "raw_G = nx.Graph() # undirected\n",
    "n = 0\n",
    "\n",
    "for i in world_data['parsed_doc']:        # sure, it's inefficient, but it will do\n",
    "    for j in world_data['parsed_doc']:\n",
    "        if i != j:\n",
    "            if not (raw_G.has_edge(j, i)):\n",
    "                sim = i.similarity(j)\n",
    "                raw_G.add_edge(i, j, weight = sim)\n",
    "                n = n + 1\n",
    "\n",
    "print(raw_G.number_of_nodes(), \"nodes, and\", raw_G.number_of_edges(), \"edges created.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "cfe3a1266e86c697bf3047a5aa95e3de62ea0175"
   },
   "cell_type": "code",
   "source": [
    "edges_to_kill = []\n",
    "min_wt = 0.94      # this is our cutoff value for a minimum edge-weight \n",
    "\n",
    "for n, nbrs in raw_G.adj.items():\n",
    "    #print(\"\\nProcessing origin-node:\", n, \"... \")\n",
    "    for nbr, eattr in nbrs.items():\n",
    "        # remove edges below a certain weight\n",
    "        data = eattr['weight']\n",
    "        if data < min_wt: \n",
    "            # print('(%.3f)' % (data))  \n",
    "            # print('(%d, %d, %.3f)' % (n, nbr, data))  \n",
    "            #print(\"\\nNode: \", n, \"\\n <-\", data, \"-> \", \"\\nNeighbour: \", nbr)\n",
    "            edges_to_kill.append((n, nbr)) \n",
    "            \n",
    "print(\"\\n\", len(edges_to_kill) / 2, \"edges to kill (of\", raw_G.number_of_edges(), \"), before de-duplicating\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "1689ead9065fec1d9114cb65b405f50b1d3d361e"
   },
   "cell_type": "code",
   "source": [
    "for u, v in edges_to_kill:\n",
    "    if raw_G.has_edge(u, v):   # catches (e.g.) those edges where we've removed them using reverse ... (v, u)\n",
    "        raw_G.remove_edge(u, v)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "b3c1f1b06199a5c0e5427f3f505596afcb452f3f"
   },
   "cell_type": "code",
   "source": [
    "strong_G = raw_G\n",
    "print(strong_G.number_of_edges())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "46527fb424e92fd5c25973a90589547e4408dab8"
   },
   "cell_type": "markdown",
   "source": [
    "We should now have a clean graph of only hi-similarity edges."
   ]
  },
  {
   "metadata": {
    "_uuid": "8281e5f32ca95cec1f24e413c0b6f6236c050135"
   },
   "cell_type": "markdown",
   "source": [
    "## Visualising the selected edges"
   ]
  },
  {
   "metadata": {
    "_uuid": "8cccd9c52607e6942e40b1cc41946b77b8790518"
   },
   "cell_type": "markdown",
   "source": [
    "NetworkX has several useful layouts implemented, but you can't beat a good spring-embedding layour (a kind of [force-directed graph](https://en.wikipedia.org/wiki/Force-directed_graph_drawing)).\n",
    "In graph terminology, what we see is:\n",
    "* a single large [component](https://en.wikipedia.org/wiki/Connected_component_(graph_theory)) at the centre,\n",
    "* with several [pendants](https://proofwiki.org/wiki/Definition:Pendant_Vertex) visible at the edges;\n",
    "* several smaller components; and \n",
    "* a peripheral cloud of [isolates](http://mathonline.wikidot.com/isolated-vertices-leaves-and-pendant-edges)\n",
    "\n",
    "Force-directed graphs are a very intuitive, satisfying, and efficient way to lay out network diagrams. Essentially, every node exerts a repulsive force on every other node. Simultaneously, every connected pair of nodes attract each other. The layout algorithm iterates, finding a layout that balances these forces."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "82da703d4f0105ceb01f643294f968b00fd85a0c"
   },
   "cell_type": "code",
   "source": [
    "nx.draw(strong_G, node_size=20, edge_color='gray')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "bb50edaeb5ca8e3ad36a32080ead2fce4b0291c4"
   },
   "cell_type": "markdown",
   "source": [
    "Visualising the whole graph, but only those links of weights above a certain cutoff, allows us to get a feel for a good cutoff level to use when visualising the structure. Having filtered out these lower-weighted links, we can clean up the graph by removing the isolates. This will enable the layout engine to show us more of the structure of the components."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "3cdb7ca95a849772eb6458b39f9023a45621629e"
   },
   "cell_type": "code",
   "source": [
    "strong_G.remove_nodes_from(list(nx.isolates(strong_G)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "a83ba84f0fae1b1b3b257884877dab79ff9557cc"
   },
   "cell_type": "markdown",
   "source": [
    "We can also tweak the layout algorithm. By, for example, changing the ideal distance at which the repulsive and attractive forces are in equilibrium. There's a good description of these forces [here](https://schneide.blog/tag/fruchterman-reingold/). This value interacts with the number of `iterations` in surprising ways."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "f2d9219385f8f6dd5517c46c51e0019af81bdca6"
   },
   "cell_type": "code",
   "source": [
    "from math import sqrt\n",
    "count = strong_G.number_of_nodes()\n",
    "equilibrium = 10 / sqrt(count)    # default for this is 1/sqrt(n), but this will 'blow out' the layout for better visibility\n",
    "pos = nx.fruchterman_reingold_layout(strong_G, k=equilibrium, iterations=300)\n",
    "nx.draw(strong_G, pos=pos, node_size=10, edge_color='gray')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "27fd370e464b85f9be017ed0449005503f0d98cc"
   },
   "cell_type": "markdown",
   "source": [
    "Of course, we can specify the layout we want to use, change colours, sizes, etc. The following cell adds the text of the tweets - which can make the layout hard to read."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "656a3f1a33e767c5987e9d3ef8feee204dddb310"
   },
   "cell_type": "code",
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 9]  # a better aspect ratio for labelled nodes\n",
    "\n",
    "nx.draw(strong_G, pos, font_size=3, node_size=50, edge_color='gray', with_labels=False)\n",
    "for p in pos:  # raise positions of the labels, relative to the nodes\n",
    "    pos[p][1] -= 0.03\n",
    "nx.draw_networkx_labels(strong_G, pos, font_size=8, font_color='k')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "2eff3d3e68e7229203a7c28c2324085c243c819f"
   },
   "cell_type": "markdown",
   "source": [
    "## Next Steps"
   ]
  },
  {
   "metadata": {
    "_uuid": "3fb7753a7d1b37334bbedffeb272516fb1dbfa56"
   },
   "cell_type": "markdown",
   "source": [
    "I hope this notebook was useful. Next:\n",
    "* I'd like to apply some keyword extraction to the tweets, to make this visualisation more useful;\n",
    "* there'll be some topic identification using gensim's implementation of LDA;\n",
    "* some more intelligent parameterisation of variables, such as allowing the minimum similarity cut-off to account for network size;\n",
    "* I'd like to apply a smarter similarity cut-off, such as Vladimir Batagelj's '[vertex islands](http://vlado.fmf.uni-lj.si/pub/networks/doc/mix/islands.pdf)' technique; and\n",
    "* I should really apply TF-IDF, if only just to see how it compares to other keyword extraction techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
